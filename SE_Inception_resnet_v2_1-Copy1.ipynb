{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#cifar-10-test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn import metrics\n",
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials,rand\n",
    "from hyperopt.mongoexp import MongoTrials\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import sobol\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "from __future__ import absolute_import,division,print_function\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from pyDOE import *\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/cluster\n",
      "defect_test1.PNG\n",
      "defect_test.PNG\n",
      "images/ball\n",
      "defect_test21.PNG\n",
      "defect_test2.PNG\n",
      "(94, 96, 3)\n",
      "(96, 90, 90, 3) +++ (96,)\n",
      "(95, 123, 3)\n",
      "(756, 90, 90, 3) +++ (756,)\n"
     ]
    }
   ],
   "source": [
    "def readData(pairpathlabel):\n",
    "    '''read image to list'''\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for filepath, label in pairpathlabel:\n",
    "        for (path,dirnames,filenames) in os.walk(filepath):\n",
    "            print(path)\n",
    "            for filename in filenames:\n",
    "                print(filename)\n",
    "                if filename.endswith('.jpg') or filename.endswith('.PNG'):\n",
    "                    img_path=path+'/'+filename\n",
    "                    img=cv2.imread(img_path)\n",
    "                    #img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "                    imgs.append(img)\n",
    "                    labels.append(label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "def onehot(numlist):\n",
    "    ''' get one hot return host matrix is len * max+1 demensions'''\n",
    "    b = np.zeros([len(numlist), max(numlist)+1])\n",
    "    b[np.arange(len(numlist)), numlist] = 1\n",
    "    return b.tolist()\n",
    "\n",
    "def getfileandlabel(filedir):\n",
    "    ''' get path and host paire and class index to name'''\n",
    "    dictdir = dict([[name, os.path.join(filedir, name)] \\\n",
    "                    for name in os.listdir(filedir) \n",
    "                    if os.path.isdir(os.path.join(filedir, name)) and not name.startswith('.')])\n",
    "                    #for (path, dirnames, _) in os.walk(filedir) for dirname in dirnames])\n",
    "    dirnamelist, dirpathlist = dictdir.keys(), dictdir.values()\n",
    "    indexlist = list(range(len(dirnamelist)))\n",
    "    return list(zip(dirpathlist, (indexlist))), dict(zip(indexlist, dirnamelist))\n",
    "\n",
    "pathlabelpair, indextoname = getfileandlabel('images')\n",
    "x, y = readData(pathlabelpair)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.5,stratify =y, random_state=0)\n",
    "\n",
    "imgs=[]\n",
    "labels=[]\n",
    "size=90\n",
    "for (img,label) in zip(X_train,y_train):\n",
    "    print(img.shape)\n",
    "    rows,cols = img.shape[:2]\n",
    "    for i in range(rows-size):\n",
    "        for j in range(cols-size):\n",
    "            imgs.append(img[i:i+size,j:j+size])\n",
    "            labels.append(label)\n",
    "            imgs.append(cv2.flip(img[i:i+size,j:j+size],1))\n",
    "            labels.append(label)\n",
    "            imgs.append(cv2.flip(img[i:i+size,j:j+size],0))\n",
    "            labels.append(label)\n",
    "            imgs.append(cv2.flip(img[i:i+size,j:j+size],-1))  \n",
    "            labels.append(label)\n",
    "\n",
    "    print(np.array(imgs).shape,\"+++\",np.array(labels).shape)\n",
    "imgs=np.array(imgs)\n",
    "labels=np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(imgs,labels, test_size=.3,stratify =labels, random_state=0)\n",
    "         \n",
    "te_imgs_1=[]\n",
    "te_labels_1=[]\n",
    "te_imgs_h1=[]\n",
    "te_labels_h1=[]\n",
    "te_imgs_v1=[]\n",
    "te_labels_v1=[]\n",
    "te_imgs_hv1=[]\n",
    "te_labels_hv1=[]\n",
    "te_imgs_2=[]\n",
    "te_labels_2=[]\n",
    "te_imgs_h2=[]\n",
    "te_labels_h2=[]\n",
    "te_imgs_v2=[]\n",
    "te_labels_v2=[]\n",
    "te_imgs_hv2=[]\n",
    "te_labels_hv2=[]\n",
    "te_imgs_3=[]\n",
    "te_labels_3=[]\n",
    "te_imgs_h3=[]\n",
    "te_labels_h3=[]\n",
    "te_imgs_v3=[]\n",
    "te_labels_v3=[]\n",
    "te_imgs_hv3=[]\n",
    "te_labels_hv3=[]\n",
    "te_imgs_4=[]\n",
    "te_labels_4=[]\n",
    "te_imgs_h4=[]\n",
    "te_labels_h4=[]\n",
    "te_imgs_v4=[]\n",
    "te_labels_v4=[]\n",
    "te_imgs_hv4=[]\n",
    "te_labels_hv4=[]\n",
    "te_imgs_5=[]\n",
    "te_labels_5=[]\n",
    "te_imgs_h5=[]\n",
    "te_labels_h5=[]\n",
    "te_imgs_v5=[]\n",
    "te_labels_v5=[]\n",
    "te_imgs_hv5=[]\n",
    "te_labels_hv5=[]\n",
    "\n",
    "size=90\n",
    "for (img,label) in zip(X_test,y_test):  \n",
    "    h,w = img.shape[:2]\n",
    "    label=label-1\n",
    "    for i in range(1,6):                            \n",
    "        if i==1:\n",
    "            te_imgs_1.append(img[0:size,0:size])\n",
    "            te_labels_1.append(label)\n",
    "            te_imgs_h1.append(cv2.flip(img[0:size,0:size],1))\n",
    "            te_labels_h1.append(label)\n",
    "            te_imgs_v1.append(cv2.flip(img[0:size,0:size],0))\n",
    "            te_labels_v1.append(label)\n",
    "            te_imgs_hv1.append(cv2.flip(img[0:size,0:size],-1)) \n",
    "            te_labels_hv1.append(label)\n",
    "        elif i==2:\n",
    "            te_imgs_2.append(img[h-size:h,0:size])\n",
    "            te_labels_2.append(label)\n",
    "            te_imgs_h2.append(cv2.flip(img[h-size:h,0:size],1))\n",
    "            te_labels_h2.append(label)\n",
    "            te_imgs_v2.append(cv2.flip(img[h-size:h,0:size],0))\n",
    "            te_labels_v2.append(label)\n",
    "            te_imgs_hv2.append(cv2.flip(img[h-size:h,0:size],-1))  \n",
    "            te_labels_hv2.append(label)\n",
    "        elif i==3:\n",
    "            te_imgs_3.append(img[h-size:h,w-size:w])\n",
    "            te_labels_3.append(label)\n",
    "            te_imgs_h3.append(cv2.flip(img[h-size:h,w-size:w],1))\n",
    "            te_labels_h3.append(label)\n",
    "            te_imgs_v3.append(cv2.flip(img[h-size:h,w-size:w],0))\n",
    "            te_labels_v3.append(label)\n",
    "            te_imgs_hv3.append(cv2.flip(img[h-size:h,w-size:w],-1)) \n",
    "            te_labels_hv3.append(label)\n",
    "        elif i==4:\n",
    "            te_imgs_4.append(img[0:size,w-size:w])\n",
    "            te_labels_4.append(label)\n",
    "            te_imgs_h4.append(cv2.flip(img[0:size,w-size:w],1))\n",
    "            te_labels_h4.append(label)\n",
    "            te_imgs_v4.append(cv2.flip(img[0:size,w-size:w],0))\n",
    "            te_labels_v4.append(label)\n",
    "            te_imgs_hv4.append(cv2.flip(img[0:size,w-size:w],-1)) \n",
    "            te_labels_hv4.append(label)\n",
    "        else :\n",
    "            hs=(h-size)//2\n",
    "            ws=(w-size)//2\n",
    "            te_imgs_5.append(img[hs:hs+size,ws:ws+size])\n",
    "            te_labels_5.append(label)\n",
    "            te_imgs_h5.append(cv2.flip(img[hs:hs+size,ws:ws+size],1))\n",
    "            te_labels_h5.append(label)\n",
    "            te_imgs_v5.append(cv2.flip(img[hs:hs+size,ws:ws+size],0))\n",
    "            te_labels_v5.append(label)\n",
    "            te_imgs_hv5.append(cv2.flip(img[hs:hs+size,ws:ws+size],-1)) \n",
    "            te_labels_hv5.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 154, 154, 3)\n",
      "2018-09-12 15:15:20\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.556250, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.778711, train_acc: 0.556250, val_loss: 0.687720, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 2,best_train_accuracy: 0.973437, best_val_precision: 0.872247\n",
      "epoch: 2/15, train_loss: 0.426266, train_acc: 0.973437, val_loss: 0.679963, val_acc: 0.872247 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 3,best_train_accuracy: 1.000000, best_val_precision: 0.872247\n",
      "epoch: 3/15, train_loss: 0.194841, train_acc: 1.000000, val_loss: 0.673068, val_acc: 0.872247 \n",
      "\n",
      "epoch--:4\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 4/15, train_loss: 0.102447, train_acc: 1.000000, val_loss: 0.549643, val_acc: 1.000000 \n",
      "\n",
      "epoch--:5\n",
      "Best number of rounds: 5,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 5/15, train_loss: 0.063321, train_acc: 1.000000, val_loss: 0.168575, val_acc: 1.000000 \n",
      "\n",
      "epoch--:6\n",
      "Best number of rounds: 6,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 6/15, train_loss: 0.045928, train_acc: 1.000000, val_loss: 0.053447, val_acc: 1.000000 \n",
      "\n",
      "epoch--:7\n",
      "Best number of rounds: 7,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 7/15, train_loss: 0.034276, train_acc: 1.000000, val_loss: 0.030233, val_acc: 1.000000 \n",
      "\n",
      "epoch--:8\n",
      "Best number of rounds: 8,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 8/15, train_loss: 0.029175, train_acc: 1.000000, val_loss: 0.022283, val_acc: 1.000000 \n",
      "\n",
      "epoch--:9\n",
      "Best number of rounds: 9,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 9/15, train_loss: 0.025056, train_acc: 1.000000, val_loss: 0.018423, val_acc: 1.000000 \n",
      "\n",
      "epoch--:10\n",
      "Best number of rounds: 10,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 10/15, train_loss: 0.022268, train_acc: 1.000000, val_loss: 0.017128, val_acc: 1.000000 \n",
      "\n",
      "epoch--:11\n",
      "Best number of rounds: 11,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 11/15, train_loss: 0.020791, train_acc: 1.000000, val_loss: 0.016415, val_acc: 1.000000 \n",
      "\n",
      "epoch--:12\n",
      "Best number of rounds: 12,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 12/15, train_loss: 0.018892, train_acc: 1.000000, val_loss: 0.016122, val_acc: 1.000000 \n",
      "\n",
      "epoch--:13\n",
      "Best number of rounds: 13,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 13/15, train_loss: 0.017824, train_acc: 1.000000, val_loss: 0.015719, val_acc: 1.000000 \n",
      "\n",
      "epoch--:14\n",
      "Best number of rounds: 14,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 14/15, train_loss: 0.015929, train_acc: 1.000000, val_loss: 0.015089, val_acc: 1.000000 \n",
      "\n",
      "epoch--:15\n",
      "Best number of rounds: 15,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 15/15, train_loss: 0.014921, train_acc: 1.000000, val_loss: 0.014418, val_acc: 1.000000 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 15:34:40\n",
      "[ True  True]\n",
      "1.0\n",
      "2018-09-12 15:34:47\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.layers import batch_norm, flatten\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "#from cifar10 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=0,1\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config = config)\n",
    "\n",
    "class_num = 2\n",
    "image_size = 90\n",
    "img_channels = 3\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "init_learning_rate = 0.001\n",
    "reduction_ratio = 4\n",
    "\n",
    "def conv_layer(input, filter, kernel, stride=1, padding='SAME', layer_name=\"conv\", activation=True):\n",
    "    with tf.name_scope(layer_name):\n",
    "        network = tf.layers.conv2d(inputs=input, use_bias=True, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
    "        if activation :\n",
    "            network = Relu(network)\n",
    "        return network\n",
    "\n",
    "def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
    "    with tf.name_scope(layer_name) :\n",
    "        return tf.layers.dense(inputs=x, use_bias=True, units=units)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def get_incoming_shape(incoming):\n",
    "    \"\"\" Returns the incoming data shape \"\"\"\n",
    "    if isinstance(incoming, tf.Tensor):\n",
    "        return incoming.get_shape().as_list()\n",
    "    elif type(incoming) in [np.array, np.ndarray, list, tuple]:\n",
    "        return np.shape(incoming)\n",
    "    else:\n",
    "        raise Exception(\"Invalid incoming layer.\")\n",
    "\n",
    "def global_avg_pool(incoming, name=\"GlobalAvgPool\"):\n",
    "    input_shape = get_incoming_shape(incoming)\n",
    "    assert len(input_shape) == 4, \"Incoming Tensor shape must be 4-D\"\n",
    "    with tf.name_scope(name):\n",
    "        inference = tf.reduce_mean(incoming, [1, 2])\n",
    "    return inference\n",
    "\n",
    "def Global_Average_Pooling(x):\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "def Max_pooling(x, pool_size=[3,3], stride=2, padding='VALID') :\n",
    "    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "def Concatenation(layers) :\n",
    "    return tf.concat(layers, axis=3)\n",
    "\n",
    "def Dropout(x, rate, training) :\n",
    "    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n",
    "\n",
    "def Evaluate(sess):\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    test_feed_dict = {\n",
    "        x: X_val,\n",
    "        label: y_val,\n",
    "        learning_rate: epoch_learning_rate,\n",
    "        training_flag: False\n",
    "    }\n",
    "\n",
    "    loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
    "\n",
    "    test_loss += loss_\n",
    "    test_acc += acc_\n",
    "\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
    "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
    "\n",
    "    return test_acc, test_loss, summary\n",
    "\n",
    "class SE_Inception_resnet_v2():\n",
    "    def __init__(self, x, training):\n",
    "        self.training = training\n",
    "        self.model = self.Build_SEnet(x)\n",
    "\n",
    "    def Stem(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            x = conv_layer(x, filter=32, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_conv1')\n",
    "            x = conv_layer(x, filter=32, kernel=[3,3], padding='VALID', layer_name=scope+'_conv2')\n",
    "            block_1 = conv_layer(x, filter=64, kernel=[3,3], layer_name=scope+'_conv3')\n",
    "\n",
    "            split_max_x = Max_pooling(block_1)\n",
    "            split_conv_x = conv_layer(block_1, filter=96, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv1')\n",
    "            x = Concatenation([split_max_x,split_conv_x])\n",
    "\n",
    "            split_conv_x1 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "            split_conv_x1 = conv_layer(split_conv_x1, filter=96, kernel=[3,3], padding='VALID', layer_name=scope+'_split_conv3')\n",
    "\n",
    "            split_conv_x2 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+'_split_conv4')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[7,1], layer_name=scope+'_split_conv5')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[1,7], layer_name=scope+'_split_conv6')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=96, kernel=[3,3], padding='VALID', layer_name=scope+'_split_conv7')\n",
    "\n",
    "            x = Concatenation([split_conv_x1,split_conv_x2])\n",
    "\n",
    "            split_conv_x = conv_layer(x, filter=192, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv8')\n",
    "            split_max_x = Max_pooling(x)\n",
    "\n",
    "            x = Concatenation([split_conv_x, split_max_x])\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Inception_resnet_A(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            init = x\n",
    "\n",
    "            split_conv_x1 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "\n",
    "            split_conv_x2 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=32, kernel=[3,3], layer_name=scope+'_split_conv3')\n",
    "\n",
    "            split_conv_x3 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+'_split_conv4')\n",
    "            split_conv_x3 = conv_layer(split_conv_x3, filter=48, kernel=[3,3], layer_name=scope+'_split_conv5')\n",
    "            split_conv_x3 = conv_layer(split_conv_x3, filter=64, kernel=[3,3], layer_name=scope+'_split_conv6')\n",
    "\n",
    "            x = Concatenation([split_conv_x1,split_conv_x2,split_conv_x3])\n",
    "            x = conv_layer(x, filter=384, kernel=[1,1], layer_name=scope+'_final_conv1', activation=False)\n",
    "\n",
    "            x = x*0.1\n",
    "            x = init + x\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Inception_resnet_B(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            init = x\n",
    "\n",
    "            split_conv_x1 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "\n",
    "            split_conv_x2 = conv_layer(x, filter=128, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=160, kernel=[1,7], layer_name=scope+'_split_conv3')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=192, kernel=[7,1], layer_name=scope+'_split_conv4')\n",
    "\n",
    "            x = Concatenation([split_conv_x1, split_conv_x2])\n",
    "            x = conv_layer(x, filter=1152, kernel=[1,1], layer_name=scope+'_final_conv1', activation=False)\n",
    "            # 1154\n",
    "            x = x * 0.1\n",
    "            x = init + x\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Inception_resnet_C(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            init = x\n",
    "\n",
    "            split_conv_x1 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "\n",
    "            split_conv_x2 = conv_layer(x, filter=192, kernel=[1, 1], layer_name=scope + '_split_conv2')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=224, kernel=[1, 3], layer_name=scope + '_split_conv3')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=256, kernel=[3, 1], layer_name=scope + '_split_conv4')\n",
    "\n",
    "            x = Concatenation([split_conv_x1,split_conv_x2])\n",
    "            x = conv_layer(x, filter=2144, kernel=[1,1], layer_name=scope+'_final_conv2', activation=False)\n",
    "            # 2048\n",
    "            x = x * 0.1\n",
    "            x = init + x\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Reduction_A(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            k = 256\n",
    "            l = 256\n",
    "            m = 384\n",
    "            n = 384\n",
    "\n",
    "            split_max_x = Max_pooling(x)\n",
    "\n",
    "            split_conv_x1 = conv_layer(x, filter=n, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv1')\n",
    "\n",
    "            split_conv_x2 = conv_layer(x, filter=k, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=l, kernel=[3,3], layer_name=scope+'_split_conv3')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=m, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv4')\n",
    "\n",
    "            x = Concatenation([split_max_x, split_conv_x1, split_conv_x2])\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Reduction_B(self, x, scope):\n",
    "        with tf.name_scope(scope) :\n",
    "            split_max_x = Max_pooling(x)\n",
    "\n",
    "            split_conv_x1 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "            split_conv_x1 = conv_layer(split_conv_x1, filter=384, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv2')\n",
    "\n",
    "            split_conv_x2 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+'_split_conv3')\n",
    "            split_conv_x2 = conv_layer(split_conv_x2, filter=288, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv4')\n",
    "\n",
    "            split_conv_x3 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+'_split_conv5')\n",
    "            split_conv_x3 = conv_layer(split_conv_x3, filter=288, kernel=[3,3], layer_name=scope+'_split_conv6')\n",
    "            split_conv_x3 = conv_layer(split_conv_x3, filter=320, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv7')\n",
    "\n",
    "            x = Concatenation([split_max_x, split_conv_x1, split_conv_x2, split_conv_x3])\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
    "        with tf.name_scope(layer_name) :\n",
    "\n",
    "\n",
    "            squeeze = Global_Average_Pooling(input_x)\n",
    "\n",
    "            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
    "            excitation = Relu(excitation)\n",
    "            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
    "            excitation = Sigmoid(excitation)\n",
    "\n",
    "            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "            scale = input_x * excitation\n",
    "\n",
    "            return scale\n",
    "\n",
    "    def Build_SEnet(self, input_x):\n",
    "        input_x = tf.pad(input_x, [[0, 0], [32, 32], [32, 32], [0, 0]])\n",
    "        # size 32 -> 96\n",
    "        print(np.shape(input_x))\n",
    "        # only cifar10 architecture\n",
    "\n",
    "        x = self.Stem(input_x, scope='stem')\n",
    "\n",
    "        for i in range(5) :\n",
    "            x = self.Inception_resnet_A(x, scope='Inception_A'+str(i))\n",
    "            channel = int(np.shape(x)[-1])\n",
    "            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_A'+str(i))\n",
    "\n",
    "        x = self.Reduction_A(x, scope='Reduction_A')\n",
    "   \n",
    "        channel = int(np.shape(x)[-1])\n",
    "        x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_A')\n",
    "\n",
    "        for i in range(10)  :\n",
    "            x = self.Inception_resnet_B(x, scope='Inception_B'+str(i))\n",
    "            channel = int(np.shape(x)[-1])\n",
    "            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_B'+str(i))\n",
    "\n",
    "        x = self.Reduction_B(x, scope='Reduction_B')\n",
    "        \n",
    "        channel = int(np.shape(x)[-1])\n",
    "        x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_B')\n",
    "\n",
    "        for i in range(5) :\n",
    "            x = self.Inception_resnet_C(x, scope='Inception_C'+str(i))\n",
    "            channel = int(np.shape(x)[-1])\n",
    "            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_C'+str(i))\n",
    "         \n",
    "            \n",
    "        # channel = int(np.shape(x)[-1])\n",
    "        # x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_C')\n",
    "        \n",
    "        x = Global_Average_Pooling(x)\n",
    "        x = Dropout(x, rate=0.2, training=self.training)\n",
    "        x = flatten(x)\n",
    "\n",
    "        x = Fully_connected(x, layer_name='final_fully_connected')\n",
    "        return x\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "train_num_examples=y_train.shape[0]\n",
    "max_steps = int(math.ceil(train_num_examples / batch_size))\n",
    "total_sample_train = max_steps * batch_size\n",
    "best = 0\n",
    "wait = 0  #counter for patience\n",
    "best_rounds =1\n",
    "counter=0\n",
    "patience=2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
    "label = tf.placeholder(tf.int32, shape=[None])\n",
    "training_flag = tf.placeholder(tf.bool)\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "logits = SE_Inception_resnet_v2(x, training=training_flag).model\n",
    "cost = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits))\n",
    "\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
    "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.cast(label,tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "top_k_op = tf.nn.in_top_k(logits, label, 1)\n",
    "\n",
    "min_fraction_of_examples_in_queue = 0.4\n",
    "min_queue_examples = int(y_train.shape[0] * min_fraction_of_examples_in_queue)\n",
    "input_queue=tf.train.slice_input_producer([X_train,y_train],shuffle=False)\n",
    "X_batch,y_batch = tf.train.batch(input_queue,batch_size=batch_size,num_threads=100,\n",
    "                                 capacity=min_queue_examples + 3 *batch_size)  \n",
    "\n",
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "# 定义会话并开始迭代训练\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "# 启动图片数据增强的线程队列\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "train_acc_all=[]\n",
    "train_loss_all=[]\n",
    "val_acc_all=[]\n",
    "val_loss_all=[]\n",
    "epoch_learning_rate = init_learning_rate\n",
    "for epoch in xrange(1,epochs+1):\n",
    "    if epoch % 30 == 0 :\n",
    "        epoch_learning_rate = epoch_learning_rate / 10\n",
    "    true_count_train = 0\n",
    "    pre_index = 0\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    for step in xrange(1,max_steps+1):\n",
    "        image_batch, label_batch = sess.run([X_batch,y_batch])\n",
    "        train_feed_dict = {\n",
    "            x: image_batch,\n",
    "            label: label_batch,\n",
    "            learning_rate: epoch_learning_rate,\n",
    "            training_flag: True\n",
    "        }\n",
    "\n",
    "        _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
    "        batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
    "        train_loss += batch_loss\n",
    "        train_acc += batch_acc\n",
    "        pre_index += batch_size\n",
    "\n",
    "    train_loss /= max_steps # average loss\n",
    "    train_acc /= max_steps # average accuracy\n",
    "    train_acc_all.append(train_acc)\n",
    "    train_loss_all.append(train_loss)\n",
    "    \n",
    "    train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
    "                                      tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
    "\n",
    "    val_acc, val_loss, val_summary = Evaluate(sess)\n",
    "    val_acc_all.append(val_acc)\n",
    "    val_loss_all.append(val_loss)\n",
    "    \n",
    "    counter +=1\n",
    "    if val_acc >= best:\n",
    "        best = val_acc\n",
    "        best_rounds=counter\n",
    "        wait = 0\n",
    "        if not os.path.exists('./model'):\n",
    "            os.makedirs('./model')\n",
    "        saver=tf.train.Saver()\n",
    "        saver.save(sess,save_path='./model/Inception_resnet_v2.ckpt')\n",
    "    else:\n",
    "        wait += 1 #incremental the number of times without improvement\n",
    "\n",
    "    line = \"epoch: %d/%d, train_loss: %f, train_acc: %f, val_loss: %f, val_acc: %f \\n\" % (\n",
    "        epoch, epochs, train_loss, train_acc, val_loss, val_acc)\n",
    "    with open('logs.txt', 'a') as f:\n",
    "        f.write(line)\n",
    "\n",
    "    if wait >= patience : #no more patience, retrieve best model\n",
    "        break\n",
    "\n",
    "print('epoch--:%d,best_rounds: %d, min_val_loss: %f, best_val_acc: %f ,best_train_acc: %f' % \n",
    "      (epoch,best_rounds,min_val_loss, best,train_acc_all[best_rounds-1]))\n",
    "\n",
    "try: \n",
    "    saver\n",
    "except NameError:\n",
    "    print ('saver_exists=False')\n",
    "else:\n",
    "    saver.restore(sess,save_path='./model/Inception_resnet_v2.ckpt')   \n",
    "print('####################################')\n",
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "te_1_logits=logits.eval(feed_dict={x: te_imgs_1,label: te_labels_1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_h1_logits=logits.eval(feed_dict={x: te_imgs_h1,label: te_labels_h1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_v1_logits=logits.eval(feed_dict={x: te_imgs_v1,label: te_labels_v1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_hv1_logits=logits.eval(feed_dict={x: te_imgs_hv1,label: te_labels_hv1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_2_logits=logits.eval(feed_dict={x: te_imgs_2,label: te_labels_2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_h2_logits=logits.eval(feed_dict={x: te_imgs_h2,label: te_labels_h2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_v2_logits=logits.eval(feed_dict={x: te_imgs_v2,label: te_labels_v2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_hv2_logits=logits.eval(feed_dict={x: te_imgs_hv2,label: te_labels_hv2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_3_logits=logits.eval(feed_dict={x: te_imgs_3,label: te_labels_3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_h3_logits=logits.eval(feed_dict={x: te_imgs_h3,label: te_labels_h3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_v3_logits=logits.eval(feed_dict={x: te_imgs_v3,label: te_labels_v3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_hv3_logits=logits.eval(feed_dict={x: te_imgs_hv3,label: te_labels_hv3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_4_logits=logits.eval(feed_dict={x: te_imgs_4,label: te_labels_4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_h4_logits=logits.eval(feed_dict={x: te_imgs_h4,label: te_labels_h4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_v4_logits=logits.eval(feed_dict={x: te_imgs_v4,label: te_labels_v4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_hv4_logits=logits.eval(feed_dict={x: te_imgs_hv4,label: te_labels_hv4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_5_logits=logits.eval(feed_dict={x: te_imgs_5,label: te_labels_5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_h5_logits=logits.eval(feed_dict={x: te_imgs_h5,label: te_labels_h5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_v5_logits=logits.eval(feed_dict={x: te_imgs_v5,label: te_labels_v5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "te_hv5_logits=logits.eval(feed_dict={x: te_imgs_hv5,label: te_labels_hv5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "\n",
    "te_logits =(te_1_logits+te_2_logits+te_3_logits+te_4_logits+te_5_logits+\n",
    "    te_h1_logits+te_h2_logits+te_h3_logits+te_h4_logits+te_h5_logits+\n",
    "    te_v1_logits+te_v2_logits+te_v3_logits+te_v4_logits+te_v5_logits+\n",
    "    te_hv1_logits+te_hv2_logits+te_hv3_logits+te_hv4_logits+te_hv5_logits)/20\n",
    "te_ture = sess.run(tf.nn.in_top_k(tf.convert_to_tensor(te_logits), y_test, 1))\n",
    "print(te_ture)\n",
    "te_true_count = np.sum(te_ture)\n",
    "te_accuracy = te_true_count / y_test.shape[0]\n",
    "print(te_accuracy)\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "sess.close()\n",
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9af49437eb3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_summary' is not defined"
     ]
    }
   ],
   "source": [
    "val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
