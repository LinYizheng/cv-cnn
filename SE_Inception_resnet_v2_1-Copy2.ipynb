{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#cifar-10-test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn import metrics\n",
    "from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials,rand\n",
    "from hyperopt.mongoexp import MongoTrials\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import sobol\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "from __future__ import absolute_import,division,print_function\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from pyDOE import *\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/cluster\n",
      "defect_test1.PNG\n",
      "defect_test.PNG\n",
      "images/ball\n",
      "defect_test21.PNG\n",
      "defect_test2.PNG\n",
      "(4,) (4,)\n",
      "(192, 90, 90) +++ (192,)\n",
      "(1512, 90, 90) +++ (1512,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def readData(pairpathlabel):\n",
    "    '''read image to list'''\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for filepath, label in pairpathlabel:\n",
    "        for (path,dirnames,filenames) in os.walk(filepath):\n",
    "            print(path)\n",
    "            for filename in filenames:\n",
    "                print(filename)\n",
    "                if filename.endswith('.jpg') or filename.endswith('.PNG'):\n",
    "                    img_path=path+'/'+filename\n",
    "                    img=cv2.imread(img_path,0)\n",
    "                    #img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "                    imgs.append(img)\n",
    "                    labels.append(label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "def onehot(numlist):\n",
    "    ''' get one hot return host matrix is len * max+1 demensions'''\n",
    "    b = np.zeros([len(numlist), max(numlist)+1])\n",
    "    b[np.arange(len(numlist)), numlist] = 1\n",
    "    return b.tolist()\n",
    "\n",
    "def getfileandlabel(filedir):\n",
    "    ''' get path and host paire and class index to name'''\n",
    "    dictdir = dict([[name, os.path.join(filedir, name)] \\\n",
    "                    for name in os.listdir(filedir) \n",
    "                    if os.path.isdir(os.path.join(filedir, name)) and not name.startswith('.')])\n",
    "                    #for (path, dirnames, _) in os.walk(filedir) for dirname in dirnames])\n",
    "    dirnamelist, dirpathlist = dictdir.keys(), dictdir.values()\n",
    "    indexlist = list(range(len(dirnamelist)))\n",
    "    return list(zip(dirpathlist, (indexlist))), dict(zip(indexlist, dirnamelist))\n",
    "    \n",
    "pathlabelpair, indextoname = getfileandlabel('images')\n",
    "x, y = readData(pathlabelpair)\n",
    "print(np.array(x).shape,y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.5,stratify =y, random_state=0)\n",
    "\n",
    "imgs=[]\n",
    "labels=[]\n",
    "size=90\n",
    "for (img,label) in zip(X_train,y_train):\n",
    "    rows,cols = img.shape[:2]\n",
    "    for i in range(rows-size):\n",
    "        for j in range(cols-size):\n",
    "            imgs.append(img[i:i+size,j:j+size])\n",
    "            labels.append(label)\n",
    "            imgs.append(cv2.flip(img[i:i+size,j:j+size],1))\n",
    "            labels.append(label)\n",
    "            imgs.append(cv2.flip(img[i:i+size,j:j+size],0))\n",
    "            labels.append(label)\n",
    "            imgs.append(cv2.flip(img[i:i+size,j:j+size],-1))  \n",
    "            labels.append(label)\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n",
    "            imgs.append(clahe.apply(img[i:i+size,j:j+size]))\n",
    "            labels.append(label)\n",
    "            imgs.append(clahe.apply(cv2.flip(img[i:i+size,j:j+size],1)))\n",
    "            labels.append(label)\n",
    "            imgs.append(clahe.apply(cv2.flip(img[i:i+size,j:j+size],0)))\n",
    "            labels.append(label)\n",
    "            imgs.append(clahe.apply(cv2.flip(img[i:i+size,j:j+size],-1)))\n",
    "            labels.append(label)\n",
    "            \n",
    "\n",
    "    print(np.array(imgs).shape,\"+++\",np.array(labels).shape)\n",
    "imgs=np.array(imgs)\n",
    "labels=np.array(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(imgs,labels, test_size=.3,stratify =labels, random_state=0)\n",
    "         \n",
    "te_imgs_1=[]\n",
    "te_labels_1=[]\n",
    "te_imgs_h1=[]\n",
    "te_labels_h1=[]\n",
    "te_imgs_v1=[]\n",
    "te_labels_v1=[]\n",
    "te_imgs_hv1=[]\n",
    "te_labels_hv1=[]\n",
    "te_imgs_2=[]\n",
    "te_labels_2=[]\n",
    "te_imgs_h2=[]\n",
    "te_labels_h2=[]\n",
    "te_imgs_v2=[]\n",
    "te_labels_v2=[]\n",
    "te_imgs_hv2=[]\n",
    "te_labels_hv2=[]\n",
    "te_imgs_3=[]\n",
    "te_labels_3=[]\n",
    "te_imgs_h3=[]\n",
    "te_labels_h3=[]\n",
    "te_imgs_v3=[]\n",
    "te_labels_v3=[]\n",
    "te_imgs_hv3=[]\n",
    "te_labels_hv3=[]\n",
    "te_imgs_4=[]\n",
    "te_labels_4=[]\n",
    "te_imgs_h4=[]\n",
    "te_labels_h4=[]\n",
    "te_imgs_v4=[]\n",
    "te_labels_v4=[]\n",
    "te_imgs_hv4=[]\n",
    "te_labels_hv4=[]\n",
    "te_imgs_5=[]\n",
    "te_labels_5=[]\n",
    "te_imgs_h5=[]\n",
    "te_labels_h5=[]\n",
    "te_imgs_v5=[]\n",
    "te_labels_v5=[]\n",
    "te_imgs_hv5=[]\n",
    "te_labels_hv5=[]\n",
    "\n",
    "size=90\n",
    "for (img,label) in zip(X_test,y_test): \n",
    "    h,w = img.shape[:2]\n",
    "    label=label-1\n",
    "    for i in range(1,6):                            \n",
    "        if i==1:\n",
    "            te_imgs_1.append(img[0:size,0:size])\n",
    "            te_labels_1.append(label)\n",
    "            te_imgs_h1.append(cv2.flip(img[0:size,0:size],1))\n",
    "            te_labels_h1.append(label)\n",
    "            te_imgs_v1.append(cv2.flip(img[0:size,0:size],0))\n",
    "            te_labels_v1.append(label)\n",
    "            te_imgs_hv1.append(cv2.flip(img[0:size,0:size],-1)) \n",
    "            te_labels_hv1.append(label)\n",
    "        elif i==2:\n",
    "            te_imgs_2.append(img[h-size:h,0:size])\n",
    "            te_labels_2.append(label)\n",
    "            te_imgs_h2.append(cv2.flip(img[h-size:h,0:size],1))\n",
    "            te_labels_h2.append(label)\n",
    "            te_imgs_v2.append(cv2.flip(img[h-size:h,0:size],0))\n",
    "            te_labels_v2.append(label)\n",
    "            te_imgs_hv2.append(cv2.flip(img[h-size:h,0:size],-1))  \n",
    "            te_labels_hv2.append(label)\n",
    "        elif i==3:\n",
    "            te_imgs_3.append(img[h-size:h,w-size:w])\n",
    "            te_labels_3.append(label)\n",
    "            te_imgs_h3.append(cv2.flip(img[h-size:h,w-size:w],1))\n",
    "            te_labels_h3.append(label)\n",
    "            te_imgs_v3.append(cv2.flip(img[h-size:h,w-size:w],0))\n",
    "            te_labels_v3.append(label)\n",
    "            te_imgs_hv3.append(cv2.flip(img[h-size:h,w-size:w],-1)) \n",
    "            te_labels_hv3.append(label)\n",
    "        elif i==4:\n",
    "            te_imgs_4.append(img[0:size,w-size:w])\n",
    "            te_labels_4.append(label)\n",
    "            te_imgs_h4.append(cv2.flip(img[0:size,w-size:w],1))\n",
    "            te_labels_h4.append(label)\n",
    "            te_imgs_v4.append(cv2.flip(img[0:size,w-size:w],0))\n",
    "            te_labels_v4.append(label)\n",
    "            te_imgs_hv4.append(cv2.flip(img[0:size,w-size:w],-1)) \n",
    "            te_labels_hv4.append(label)\n",
    "        else :\n",
    "            hs=(h-size)//2\n",
    "            ws=(w-size)//2\n",
    "            te_imgs_5.append(img[hs:hs+size,ws:ws+size])\n",
    "            te_labels_5.append(label)\n",
    "            te_imgs_h5.append(cv2.flip(img[hs:hs+size,ws:ws+size],1))\n",
    "            te_labels_h5.append(label)\n",
    "            te_imgs_v5.append(cv2.flip(img[hs:hs+size,ws:ws+size],0))\n",
    "            te_labels_v5.append(label)\n",
    "            te_imgs_hv5.append(cv2.flip(img[hs:hs+size,ws:ws+size],-1)) \n",
    "            te_labels_hv5.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-28 15:32:39\n",
      "{'learning_rate': 0.09129130831138302, 'weight_decay': 0.008936897183315345, 'batch_size': 253, 'momentum': 0.2861938245723319}\n",
      "(?, 154, 154, 1)\n",
      "2018-09-28 15:33:11\n",
      "epoch--:3,best_rounds: 1, min_val_loss: 0.653341, best_val_acc: 0.872247 ,best_train_acc: 0.961265\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-28 15:47:09\n",
      "[ True False]\n",
      "0.5\n",
      "{'learning_rate': 0.06709673801330117, 'weight_decay': 0.006727770563230353, 'batch_size': 142, 'momentum': 0.9497824137110675}\n",
      "(?, 154, 154, 1)\n",
      "2018-09-28 15:47:51\n",
      "epoch--:3,best_rounds: 1, min_val_loss: 0.598758, best_val_acc: 0.872247 ,best_train_acc: 0.957746\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-28 16:00:01\n",
      "[ True False]\n",
      "0.5\n",
      "{'learning_rate': 0.08014761554759614, 'weight_decay': 0.0008586781478701359, 'batch_size': 29, 'momentum': 0.6768825652374666}\n",
      "(?, 154, 154, 1)\n",
      "2018-09-28 16:01:02\n",
      "epoch--:8,best_rounds: 6, min_val_loss: 0.000727, best_val_acc: 1.000000 ,best_train_acc: 1.000000\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-28 16:37:22\n",
      "[ True  True]\n",
      "1.0\n",
      "{'learning_rate': 0.06914416493471434, 'weight_decay': 0.007555403394131002, 'batch_size': 113, 'momentum': 0.7087129735983153}\n",
      "(?, 154, 154, 1)\n",
      "2018-09-28 16:38:01\n",
      "epoch--:10,best_rounds: 8, min_val_loss: 0.432742, best_val_acc: 0.944934 ,best_train_acc: 1.000000\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-28 17:20:27\n",
      "[ True  True]\n",
      "1.0\n",
      "{'learning_rate': 0.06384872239680871, 'weight_decay': 0.008166766223376969, 'batch_size': 117, 'momentum': 0.6200791164571452}\n",
      "(?, 154, 154, 1)\n",
      "2018-09-28 17:21:12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-48108be132f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0mdoe_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSE_Inception_resnet_v2_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoe_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoe_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"best:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, does, doe_num, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, does, doe_num, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    323\u001b[0m                     verbose=verbose)\n\u001b[1;32m    324\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-48108be132f6>\u001b[0m in \u001b[0;36mSE_Inception_resnet_v2_1\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    369\u001b[0m             }\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'momentum': hp.uniform('momentum', 0.1,0.95),   \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.0001,0.1),\n",
    "    'weight_decay': hp.uniform('weight_decay', 0.00001,0.01),\n",
    "    'batch_size' : hp.choice('batch_size', range(28,256))\n",
    "       }\n",
    "def SE_Inception_resnet_v2_1(params):\n",
    "    import tensorflow as tf\n",
    "    #from tflearn.layers.conv import global_avg_pool\n",
    "    from tensorflow.contrib.layers import batch_norm, flatten\n",
    "    from tensorflow.contrib.framework import arg_scope\n",
    "    #from cifar10 import *\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import sys\n",
    "    import time\n",
    "    import pickle\n",
    "    import random\n",
    "\n",
    "    class_num = 2\n",
    "    image_size = 90\n",
    "    img_channels = 1\n",
    "    weight_decay = params['weight_decay']\n",
    "    momentum =params['momentum']\n",
    "    init_learning_rate = params['learning_rate']\n",
    "    reduction_ratio = 16\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    def conv_layer(input, filter, kernel, stride=1, padding='SAME', layer_name=\"conv\", activation=True):\n",
    "        with tf.name_scope(layer_name):\n",
    "            network = tf.layers.conv2d(inputs=input, use_bias=True, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n",
    "            if activation :\n",
    "                network = Relu(network)\n",
    "            return network\n",
    "\n",
    "    def Fully_connected(x, units=class_num, layer_name='fully_connected') :\n",
    "        with tf.name_scope(layer_name) :\n",
    "            return tf.layers.dense(inputs=x, use_bias=True, units=units)\n",
    "\n",
    "    def Relu(x):\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "    def Sigmoid(x):\n",
    "        return tf.nn.sigmoid(x)\n",
    "\n",
    "    def get_incoming_shape(incoming):\n",
    "        \"\"\" Returns the incoming data shape \"\"\"\n",
    "        if isinstance(incoming, tf.Tensor):\n",
    "            return incoming.get_shape().as_list()\n",
    "        elif type(incoming) in [np.array, np.ndarray, list, tuple]:\n",
    "            return np.shape(incoming)\n",
    "        else:\n",
    "            raise Exception(\"Invalid incoming layer.\")\n",
    "\n",
    "    def global_avg_pool(incoming, name=\"GlobalAvgPool\"):\n",
    "        input_shape = get_incoming_shape(incoming)\n",
    "        assert len(input_shape) == 4, \"Incoming Tensor shape must be 4-D\"\n",
    "        with tf.name_scope(name):\n",
    "            inference = tf.reduce_mean(incoming, [1, 2])\n",
    "        return inference\n",
    "\n",
    "    def Global_Average_Pooling(x):\n",
    "        return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "    def Max_pooling(x, pool_size=[3,3], stride=2, padding='VALID') :\n",
    "        return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "    def Batch_Normalization(x, training, scope):\n",
    "        with arg_scope([batch_norm],\n",
    "                       scope=scope,\n",
    "                       updates_collections=None,\n",
    "                       decay=0.9,\n",
    "                       center=True,\n",
    "                       scale=True,\n",
    "                       zero_debias_moving_mean=True) :\n",
    "            return tf.cond(training,\n",
    "                           lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                           lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "    def Concatenation(layers) :\n",
    "        return tf.concat(layers, axis=3)\n",
    "\n",
    "    def Dropout(x, rate, training) :\n",
    "        return tf.layers.dropout(inputs=x, rate=rate, training=training)\n",
    "\n",
    "    def Evaluate(sess):\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        test_feed_dict = {\n",
    "            x: X_val,\n",
    "            label: y_val,\n",
    "            learning_rate: epoch_learning_rate,\n",
    "            training_flag: False\n",
    "        }\n",
    "\n",
    "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
    "\n",
    "        val_loss += loss_\n",
    "        val_acc += acc_\n",
    "        return val_acc, val_loss\n",
    "\n",
    "    class SE_Inception_resnet_v2():\n",
    "        def __init__(self, x, training):\n",
    "            self.training = training\n",
    "            self.model = self.Build_SEnet(x)\n",
    "\n",
    "        def Stem(self, x, scope):\n",
    "            with tf.name_scope(scope) :\n",
    "                x = conv_layer(x, filter=32, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_conv1')\n",
    "                x = conv_layer(x, filter=32, kernel=[3,3], padding='VALID', layer_name=scope+'_conv2')\n",
    "                block_1 = conv_layer(x, filter=64, kernel=[3,3], layer_name=scope+'_conv3')\n",
    "\n",
    "                split_max_x = Max_pooling(block_1)\n",
    "                split_conv_x = conv_layer(block_1, filter=96, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv1')\n",
    "                x = Concatenation([split_max_x,split_conv_x])\n",
    "\n",
    "                split_conv_x1 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "                split_conv_x1 = conv_layer(split_conv_x1, filter=96, kernel=[3,3], padding='VALID', layer_name=scope+'_split_conv3')\n",
    "\n",
    "                split_conv_x2 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+'_split_conv4')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[7,1], layer_name=scope+'_split_conv5')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[1,7], layer_name=scope+'_split_conv6')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=96, kernel=[3,3], padding='VALID', layer_name=scope+'_split_conv7')\n",
    "\n",
    "                x = Concatenation([split_conv_x1,split_conv_x2])\n",
    "\n",
    "                split_conv_x = conv_layer(x, filter=192, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv8')\n",
    "                split_max_x = Max_pooling(x)\n",
    "\n",
    "                x = Concatenation([split_conv_x, split_max_x])\n",
    "\n",
    "                x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "                x = Relu(x)\n",
    "\n",
    "                return x\n",
    "\n",
    "        def Inception_resnet_A(self, x, scope):\n",
    "            with tf.name_scope(scope) :\n",
    "                init = x\n",
    "\n",
    "                split_conv_x1 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "\n",
    "                split_conv_x2 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=32, kernel=[3,3], layer_name=scope+'_split_conv3')\n",
    "\n",
    "                split_conv_x3 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+'_split_conv4')\n",
    "                split_conv_x3 = conv_layer(split_conv_x3, filter=48, kernel=[3,3], layer_name=scope+'_split_conv5')\n",
    "                split_conv_x3 = conv_layer(split_conv_x3, filter=64, kernel=[3,3], layer_name=scope+'_split_conv6')\n",
    "\n",
    "                x = Concatenation([split_conv_x1,split_conv_x2,split_conv_x3])\n",
    "                x = conv_layer(x, filter=384, kernel=[1,1], layer_name=scope+'_final_conv1', activation=False)\n",
    "\n",
    "                x = x*0.1\n",
    "                x = init + x\n",
    "\n",
    "                x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "                x = Relu(x)\n",
    "\n",
    "                return x\n",
    "\n",
    "        def Inception_resnet_B(self, x, scope):\n",
    "            with tf.name_scope(scope) :\n",
    "                init = x\n",
    "\n",
    "                split_conv_x1 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "\n",
    "                split_conv_x2 = conv_layer(x, filter=128, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=160, kernel=[1,7], layer_name=scope+'_split_conv3')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=192, kernel=[7,1], layer_name=scope+'_split_conv4')\n",
    "\n",
    "                x = Concatenation([split_conv_x1, split_conv_x2])\n",
    "                x = conv_layer(x, filter=1152, kernel=[1,1], layer_name=scope+'_final_conv1', activation=False)\n",
    "                # 1154\n",
    "                x = x * 0.1\n",
    "                x = init + x\n",
    "\n",
    "                x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "                x = Relu(x)\n",
    "\n",
    "                return x\n",
    "\n",
    "        def Inception_resnet_C(self, x, scope):\n",
    "            with tf.name_scope(scope) :\n",
    "                init = x\n",
    "\n",
    "                split_conv_x1 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "\n",
    "                split_conv_x2 = conv_layer(x, filter=192, kernel=[1, 1], layer_name=scope + '_split_conv2')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=224, kernel=[1, 3], layer_name=scope + '_split_conv3')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=256, kernel=[3, 1], layer_name=scope + '_split_conv4')\n",
    "\n",
    "                x = Concatenation([split_conv_x1,split_conv_x2])\n",
    "                x = conv_layer(x, filter=2144, kernel=[1,1], layer_name=scope+'_final_conv2', activation=False)\n",
    "                # 2048\n",
    "                x = x * 0.1\n",
    "                x = init + x\n",
    "\n",
    "                x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "                x = Relu(x)\n",
    "\n",
    "                return x\n",
    "\n",
    "        def Reduction_A(self, x, scope):\n",
    "            with tf.name_scope(scope) :\n",
    "                k = 256\n",
    "                l = 256\n",
    "                m = 384\n",
    "                n = 384\n",
    "\n",
    "                split_max_x = Max_pooling(x)\n",
    "\n",
    "                split_conv_x1 = conv_layer(x, filter=n, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv1')\n",
    "\n",
    "                split_conv_x2 = conv_layer(x, filter=k, kernel=[1,1], layer_name=scope+'_split_conv2')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=l, kernel=[3,3], layer_name=scope+'_split_conv3')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=m, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv4')\n",
    "\n",
    "                x = Concatenation([split_max_x, split_conv_x1, split_conv_x2])\n",
    "\n",
    "                x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "                x = Relu(x)\n",
    "\n",
    "                return x\n",
    "\n",
    "        def Reduction_B(self, x, scope):\n",
    "            with tf.name_scope(scope) :\n",
    "                split_max_x = Max_pooling(x)\n",
    "\n",
    "                split_conv_x1 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+'_split_conv1')\n",
    "                split_conv_x1 = conv_layer(split_conv_x1, filter=384, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv2')\n",
    "\n",
    "                split_conv_x2 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+'_split_conv3')\n",
    "                split_conv_x2 = conv_layer(split_conv_x2, filter=288, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv4')\n",
    "\n",
    "                split_conv_x3 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+'_split_conv5')\n",
    "                split_conv_x3 = conv_layer(split_conv_x3, filter=288, kernel=[3,3], layer_name=scope+'_split_conv6')\n",
    "                split_conv_x3 = conv_layer(split_conv_x3, filter=320, kernel=[3,3], stride=2, padding='VALID', layer_name=scope+'_split_conv7')\n",
    "\n",
    "                x = Concatenation([split_max_x, split_conv_x1, split_conv_x2, split_conv_x3])\n",
    "\n",
    "                x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "                x = Relu(x)\n",
    "\n",
    "                return x\n",
    "\n",
    "        def Squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n",
    "            with tf.name_scope(layer_name) :\n",
    "\n",
    "                squeeze = Global_Average_Pooling(input_x)\n",
    "\n",
    "                excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
    "                excitation = Relu(excitation)\n",
    "                excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
    "                excitation = Sigmoid(excitation)\n",
    "\n",
    "                excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "                scale = input_x * excitation\n",
    "\n",
    "                return scale\n",
    "\n",
    "        def Build_SEnet(self, input_x):\n",
    "            input_x = tf.pad(input_x, [[0, 0], [32, 32], [32, 32], [0, 0]])\n",
    "            # size 32 -> 96\n",
    "            print(np.shape(input_x))\n",
    "            # only cifar10 architecture\n",
    "\n",
    "            x = self.Stem(input_x, scope='stem')\n",
    "\n",
    "            for i in range(5) :\n",
    "                x = self.Inception_resnet_A(x, scope='Inception_A'+str(i))\n",
    "                channel = int(np.shape(x)[-1])\n",
    "                x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_A'+str(i))\n",
    "\n",
    "            x = self.Reduction_A(x, scope='Reduction_A')\n",
    "\n",
    "            channel = int(np.shape(x)[-1])\n",
    "            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_A')\n",
    "\n",
    "            for i in range(10)  :\n",
    "                x = self.Inception_resnet_B(x, scope='Inception_B'+str(i))\n",
    "                channel = int(np.shape(x)[-1])\n",
    "                x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_B'+str(i))\n",
    "\n",
    "            x = self.Reduction_B(x, scope='Reduction_B')\n",
    "\n",
    "            channel = int(np.shape(x)[-1])\n",
    "            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_B')\n",
    "\n",
    "            for i in range(5) :\n",
    "                x = self.Inception_resnet_C(x, scope='Inception_C'+str(i))\n",
    "                channel = int(np.shape(x)[-1])\n",
    "                x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_C'+str(i))\n",
    "\n",
    "            # channel = int(np.shape(x)[-1])\n",
    "            # x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name='SE_C')\n",
    "\n",
    "            x = Global_Average_Pooling(x)\n",
    "            x = Dropout(x, rate=0.2, training=self.training)\n",
    "            x = flatten(x)\n",
    "\n",
    "            x = Fully_connected(x, layer_name='final_fully_connected')\n",
    "            return x\n",
    "\n",
    "    print(params)\n",
    "    epochs = 15\n",
    "    train_num_examples=y_train.shape[0]\n",
    "    max_steps = int(math.ceil(train_num_examples / batch_size))\n",
    "    total_sample_train = max_steps * batch_size\n",
    "    best = 0\n",
    "    min_val_loss = 1\n",
    "    wait = 0  #counter for patience\n",
    "    best_rounds =1\n",
    "    counter=0\n",
    "    patience=2\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, image_size, image_size])\n",
    "    x1 = tf.reshape(x, [-1,90,90,1])\n",
    "    label = tf.placeholder(tf.int32, shape=[None])\n",
    "    training_flag = tf.placeholder(tf.bool)\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    logits = SE_Inception_resnet_v2(x1, training=training_flag).model\n",
    "    cost = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits))\n",
    "\n",
    "    l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
    "    train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.cast(label,tf.int64))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    top_k_op = tf.nn.in_top_k(logits, label, 1)\n",
    "\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    min_queue_examples = int(y_train.shape[0] * min_fraction_of_examples_in_queue)\n",
    "    input_queue=tf.train.slice_input_producer([X_train,y_train],shuffle=False)\n",
    "    X_batch,y_batch = tf.train.batch(input_queue,batch_size=batch_size,num_threads=100,\n",
    "                                     capacity=min_queue_examples + 3 *batch_size)  \n",
    "\n",
    "    print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "    # 定义会话并开始迭代训练\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    # 启动图片数据增强的线程队列\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    train_acc_all=[]\n",
    "    train_loss_all=[]\n",
    "    val_acc_all=[]\n",
    "    val_loss_all=[]\n",
    "    epoch_learning_rate = init_learning_rate\n",
    "    for epoch in xrange(1,epochs+1):\n",
    "        if epoch % 30 == 0 :\n",
    "            epoch_learning_rate = epoch_learning_rate / 10\n",
    "        true_count_train = 0\n",
    "        pre_index = 0\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        for step in xrange(1,max_steps+1):\n",
    "            image_batch, label_batch = sess.run([X_batch,y_batch])\n",
    "            train_feed_dict = {\n",
    "                x: image_batch,\n",
    "                label: label_batch,\n",
    "                learning_rate: epoch_learning_rate,\n",
    "                training_flag: True\n",
    "            }\n",
    "\n",
    "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
    "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
    "            train_loss += batch_loss\n",
    "            train_acc += batch_acc\n",
    "            pre_index += batch_size\n",
    "\n",
    "        train_loss /= max_steps # average loss\n",
    "        train_acc /= max_steps # average accuracy\n",
    "        train_acc_all.append(train_acc)\n",
    "        train_loss_all.append(train_loss)\n",
    "\n",
    "        val_acc, val_loss = Evaluate(sess)\n",
    "        val_acc_all.append(val_acc)\n",
    "        val_loss_all.append(val_loss)\n",
    "\n",
    "        counter +=1\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best = val_acc\n",
    "            best_rounds=counter\n",
    "            wait = 0\n",
    "            if not os.path.exists('./model'):\n",
    "                os.makedirs('./model')\n",
    "            saver=tf.train.Saver()\n",
    "            saver.save(sess,save_path='./model/Inception_resnet_v2.ckpt')\n",
    "        else:\n",
    "            wait += 1 #incremental the number of times without improvement\n",
    "\n",
    "        line = \"epoch: %d/%d, train_loss: %f, train_acc: %f, val_loss: %f, val_acc: %f \\n\" % (\n",
    "            epoch, epochs, train_loss, train_acc, val_loss, val_acc)\n",
    "        with open('logs.txt', 'a') as f:\n",
    "            f.write(line)\n",
    "            \n",
    "        if wait >= patience : #no more patience, retrieve best model\n",
    "            break\n",
    "            \n",
    "    print('epoch--:%d,best_rounds: %d, min_val_loss: %f, best_val_acc: %f ,best_train_acc: %f' % \n",
    "          (epoch,best_rounds,min_val_loss, best,train_acc_all[best_rounds-1]))\n",
    "    \n",
    "    try: \n",
    "        saver\n",
    "    except NameError:\n",
    "        print ('saver_exists=False')\n",
    "    else:\n",
    "        saver.restore(sess,save_path='./model/Inception_resnet_v2.ckpt')   \n",
    "    print('####################################')\n",
    "    print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "    te_1_logits=logits.eval(feed_dict={x: te_imgs_1,label: te_labels_1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_h1_logits=logits.eval(feed_dict={x: te_imgs_h1,label: te_labels_h1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_v1_logits=logits.eval(feed_dict={x: te_imgs_v1,label: te_labels_v1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_hv1_logits=logits.eval(feed_dict={x: te_imgs_hv1,label: te_labels_hv1,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_2_logits=logits.eval(feed_dict={x: te_imgs_2,label: te_labels_2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_h2_logits=logits.eval(feed_dict={x: te_imgs_h2,label: te_labels_h2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_v2_logits=logits.eval(feed_dict={x: te_imgs_v2,label: te_labels_v2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_hv2_logits=logits.eval(feed_dict={x: te_imgs_hv2,label: te_labels_hv2,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_3_logits=logits.eval(feed_dict={x: te_imgs_3,label: te_labels_3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_h3_logits=logits.eval(feed_dict={x: te_imgs_h3,label: te_labels_h3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_v3_logits=logits.eval(feed_dict={x: te_imgs_v3,label: te_labels_v3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_hv3_logits=logits.eval(feed_dict={x: te_imgs_hv3,label: te_labels_hv3,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_4_logits=logits.eval(feed_dict={x: te_imgs_4,label: te_labels_4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_h4_logits=logits.eval(feed_dict={x: te_imgs_h4,label: te_labels_h4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_v4_logits=logits.eval(feed_dict={x: te_imgs_v4,label: te_labels_v4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_hv4_logits=logits.eval(feed_dict={x: te_imgs_hv4,label: te_labels_hv4,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_5_logits=logits.eval(feed_dict={x: te_imgs_5,label: te_labels_5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_h5_logits=logits.eval(feed_dict={x: te_imgs_h5,label: te_labels_h5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_v5_logits=logits.eval(feed_dict={x: te_imgs_v5,label: te_labels_v5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "    te_hv5_logits=logits.eval(feed_dict={x: te_imgs_hv5,label: te_labels_hv5,learning_rate: epoch_learning_rate,training_flag: False})\n",
    "\n",
    "    te_logits =(te_1_logits+te_2_logits+te_3_logits+te_4_logits+te_5_logits+\n",
    "        te_h1_logits+te_h2_logits+te_h3_logits+te_h4_logits+te_h5_logits+\n",
    "        te_v1_logits+te_v2_logits+te_v3_logits+te_v4_logits+te_v5_logits+\n",
    "        te_hv1_logits+te_hv2_logits+te_hv3_logits+te_hv4_logits+te_hv5_logits)/20\n",
    "    te_ture = sess.run(tf.nn.in_top_k(tf.convert_to_tensor(te_logits), y_test, 1))\n",
    "    print(te_ture)\n",
    "    te_true_count = np.sum(te_ture)\n",
    "    te_accuracy = te_true_count / y_test.shape[0]\n",
    "    print(te_accuracy)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n",
    "    return {'loss': val_acc*(-1),'test_accuracy':te_accuracy,'val_accuracy':best,'train_accuracy':train_acc_all[best_rounds-1],'best_rounds':best_rounds,'status': STATUS_OK}\n",
    "    print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "    \n",
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "max_evals=20\n",
    "doe_num=10\n",
    "trials = Trials()\n",
    "best =  fmin(SE_Inception_resnet_v2_1, space, algo=tpe.suggest, trials=trials, verbose=0, max_evals=max_evals, doe_num=doe_num)\n",
    "print (\"best:\",best)\n",
    "print (trials.best_trial)\n",
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-12 17:14:10\n",
      "{'learning_rate': 0.06717472444022282, 'weight_decay': 0.002658981450422523, 'batch_size': 106, 'momentum': 0.5367455912002034}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:14:32\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.941509, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.287092, train_acc: 0.941509, val_loss: 0.656459, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 1,best_train_accuracy: 0.941509, best_val_precision: 0.872247\n",
      "epoch: 2/15, train_loss: 0.013249, train_acc: 1.000000, val_loss: 0.652289, val_acc: 0.872247 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 1,best_train_accuracy: 0.941509, best_val_precision: 0.872247\n",
      "epoch: 3/15, train_loss: 0.005578, train_acc: 1.000000, val_loss: 0.650332, val_acc: 0.872247 \n",
      "\n",
      "epoch--:4\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 4/15, train_loss: 0.004055, train_acc: 1.000000, val_loss: 0.590903, val_acc: 1.000000 \n",
      "\n",
      "epoch--:5\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 5/15, train_loss: 0.003263, train_acc: 1.000000, val_loss: 0.255008, val_acc: 1.000000 \n",
      "\n",
      "epoch--:6\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 6/15, train_loss: 0.002777, train_acc: 1.000000, val_loss: 0.061995, val_acc: 1.000000 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 17:21:41\n",
      "[ True  True]\n",
      "1.0\n",
      "{'learning_rate': 0.026014558018577907, 'weight_decay': 0.00515753935086258, 'batch_size': 100, 'momentum': 0.9114079335479521}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:22:05\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.943333, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.239122, train_acc: 0.943333, val_loss: 0.655440, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 1,best_train_accuracy: 0.943333, best_val_precision: 0.872247\n",
      "epoch: 2/15, train_loss: 0.006979, train_acc: 1.000000, val_loss: 0.641658, val_acc: 0.872247 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 3,best_train_accuracy: 1.000000, best_val_precision: 0.973568\n",
      "epoch: 3/15, train_loss: 0.001809, train_acc: 1.000000, val_loss: 0.090297, val_acc: 0.973568 \n",
      "\n",
      "epoch--:4\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 4/15, train_loss: 0.000989, train_acc: 1.000000, val_loss: 0.032282, val_acc: 1.000000 \n",
      "\n",
      "epoch--:5\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 5/15, train_loss: 0.000418, train_acc: 1.000000, val_loss: 0.002132, val_acc: 1.000000 \n",
      "\n",
      "epoch--:6\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 6/15, train_loss: 0.000309, train_acc: 1.000000, val_loss: 0.001400, val_acc: 1.000000 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 17:30:08\n",
      "[ True  True]\n",
      "1.0\n",
      "{'learning_rate': 0.04463426721038167, 'weight_decay': 0.00800508872582295, 'batch_size': 28, 'momentum': 0.8306311594321598}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:30:37\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.986842, best_val_precision: 1.000000\n",
      "epoch: 1/15, train_loss: 0.085273, train_acc: 0.986842, val_loss: 0.458524, val_acc: 1.000000 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 1,best_train_accuracy: 0.986842, best_val_precision: 1.000000\n",
      "epoch: 2/15, train_loss: 0.001323, train_acc: 1.000000, val_loss: 0.509179, val_acc: 0.986784 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 1,best_train_accuracy: 0.986842, best_val_precision: 1.000000\n",
      "epoch: 3/15, train_loss: 0.001238, train_acc: 1.000000, val_loss: 0.572186, val_acc: 0.828194 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 17:34:51\n",
      "[ True  True]\n",
      "1.0\n",
      "{'learning_rate': 0.02973357959007051, 'weight_decay': 0.0009351301653411916, 'batch_size': 213, 'momentum': 0.3880964615221216}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:35:17\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.953052, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.474261, train_acc: 0.953052, val_loss: 0.673165, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 1,best_train_accuracy: 0.953052, best_val_precision: 0.872247\n",
      "epoch: 2/15, train_loss: 0.077258, train_acc: 1.000000, val_loss: 0.668606, val_acc: 0.872247 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 1,best_train_accuracy: 0.953052, best_val_precision: 0.872247\n",
      "epoch: 3/15, train_loss: 0.032746, train_acc: 1.000000, val_loss: 0.666487, val_acc: 0.872247 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 17:40:40\n",
      "[ True False]\n",
      "0.5\n",
      "{'learning_rate': 0.019191456254268314, 'weight_decay': 0.008693716960029992, 'batch_size': 109, 'momentum': 0.35596121847890194}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:41:01\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.972477, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.370746, train_acc: 0.972477, val_loss: 0.673292, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 1,best_train_accuracy: 0.972477, best_val_precision: 0.872247\n",
      "epoch: 2/15, train_loss: 0.064365, train_acc: 1.000000, val_loss: 0.669336, val_acc: 0.872247 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 1,best_train_accuracy: 0.972477, best_val_precision: 0.872247\n",
      "epoch: 3/15, train_loss: 0.033077, train_acc: 1.000000, val_loss: 0.666970, val_acc: 0.872247 \n",
      "\n",
      "epoch--:4\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 4/15, train_loss: 0.023773, train_acc: 1.000000, val_loss: 0.583961, val_acc: 1.000000 \n",
      "\n",
      "epoch--:5\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 5/15, train_loss: 0.018851, train_acc: 1.000000, val_loss: 0.214505, val_acc: 1.000000 \n",
      "\n",
      "epoch--:6\n",
      "Best number of rounds: 4,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 6/15, train_loss: 0.015528, train_acc: 1.000000, val_loss: 0.058960, val_acc: 1.000000 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 17:48:19\n",
      "[ True  True]\n",
      "1.0\n",
      "{'learning_rate': 0.023378505858644216, 'weight_decay': 0.002266394114670839, 'batch_size': 153, 'momentum': 0.2250139896782316}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:48:47\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.988562, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.382971, train_acc: 0.988562, val_loss: 0.675968, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 1,best_train_accuracy: 0.988562, best_val_precision: 0.872247\n",
      "epoch: 2/15, train_loss: 0.085971, train_acc: 1.000000, val_loss: 0.671388, val_acc: 0.872247 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 1,best_train_accuracy: 0.988562, best_val_precision: 0.872247\n",
      "epoch: 3/15, train_loss: 0.044375, train_acc: 1.000000, val_loss: 0.668537, val_acc: 0.872247 \n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./model/Inception_resnet_v2.ckpt\n",
      "####################################\n",
      "2018-09-12 17:53:47\n",
      "[ True False]\n",
      "0.5\n",
      "{'learning_rate': 0.010464812871156463, 'weight_decay': 0.0021215338558933307, 'batch_size': 68, 'momentum': 0.8751220907832864}\n",
      "(?, 154, 154, 3)\n",
      "2018-09-12 17:54:10\n",
      "epoch--:1\n",
      "Best number of rounds: 1,best_train_accuracy: 0.961397, best_val_precision: 0.872247\n",
      "epoch: 1/15, train_loss: 0.256025, train_acc: 0.961397, val_loss: 0.663653, val_acc: 0.872247 \n",
      "\n",
      "epoch--:2\n",
      "Best number of rounds: 2,best_train_accuracy: 1.000000, best_val_precision: 0.986784\n",
      "epoch: 2/15, train_loss: 0.015512, train_acc: 1.000000, val_loss: 0.623887, val_acc: 0.986784 \n",
      "\n",
      "epoch--:3\n",
      "Best number of rounds: 3,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 3/15, train_loss: 0.004538, train_acc: 1.000000, val_loss: 0.010790, val_acc: 1.000000 \n",
      "\n",
      "epoch--:4\n",
      "Best number of rounds: 3,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 4/15, train_loss: 0.002318, train_acc: 1.000000, val_loss: 0.001483, val_acc: 1.000000 \n",
      "\n",
      "epoch--:5\n",
      "Best number of rounds: 3,best_train_accuracy: 1.000000, best_val_precision: 1.000000\n",
      "epoch: 5/15, train_loss: 0.001646, train_acc: 1.000000, val_loss: 0.001252, val_acc: 1.000000 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e41916e00531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoe_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSE_Inception_resnet_v2_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoe_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoe_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"best:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, does, doe_num, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, does, doe_num, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    323\u001b[0m                     verbose=verbose)\n\u001b[1;32m    324\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-875cc3e42669>\u001b[0m in \u001b[0;36mSE_Inception_resnet_v2_1\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    367\u001b[0m             }\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))\n",
    "max_evals=20\n",
    "doe_num=10\n",
    "trials = Trials()\n",
    "best =  fmin(SE_Inception_resnet_v2_1, space, algo=tpe.suggest, trials=trials, verbose=0, max_evals=max_evals, doe_num=doe_num)\n",
    "print (\"best:\",best)\n",
    "print (trials.best_trial)\n",
    "print (time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9af49437eb3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_summary' is not defined"
     ]
    }
   ],
   "source": [
    "val_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
